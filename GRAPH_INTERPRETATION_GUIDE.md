# Graph Interpretation Guide: Understanding Comparison Results

This guide explains all the graphs generated by `compare_recovery_methods.py` when comparing the three skill parameter recovery methods (Global Optimization, Sliding Window, and Fast Method).

## Overview

When you run the comparison script, it generates **7 types of graphs** saved as high-resolution PNG files (300 DPI) in `trajectory_comparisons/{scenario}/`. Each graph provides different insights into how well each recovery method performs.

## Graph Files Generated

1. **trajectory_spatial_comparison.png** - Visual trajectory overlays
2. **smoothness_metrics_comparison.png** - Smoothness metric bar charts
3. **matching_metrics_comparison.png** - Trajectory matching metric bar charts
4. **continuity_metrics_comparison.png** - Segment continuity metric bar charts
5. **radar_chart_comparison.png** - Multi-metric radar charts
6. **metrics_over_trajectories.png** - Metrics variation across trajectory segments
7. **summary_comparison.png** - Overall performance summary

---

## 1. Trajectory Spatial Comparison

**File:** `trajectory_spatial_comparison.png`

### What It Shows

A 2×2 grid showing 4 sample trajectories with all methods overlaid on the same spatial plot.

### Visual Elements

- **Black line (thick)**: Original reference trajectory (ground truth)
- **Blue line**: Global Optimization method
- **Red line**: Sliding Window method
- **Green line**: Fast Method
- **Black circle (●)**: Start point of trajectory
- **Black star (★)**: End point of trajectory

### How to Interpret

1. **Closeness to Original**: Methods that closely follow the black line are more accurate
2. **Smoothness**: Look for jagged vs smooth curves - smoother is generally better
3. **Endpoint Accuracy**: Check if all methods reach the same endpoint (black star)
4. **Shape Matching**: See if the overall trajectory shape matches the original

### What to Look For

✅ **Good signs:**
- All colored lines closely follow the black reference line
- Smooth curves without sudden jumps
- All methods converge to the same endpoint

❌ **Warning signs:**
- Large gaps between colored lines and black line
- Jagged or oscillating trajectories
- Methods ending at different points

### Example Interpretation

If the blue (Global) line is closest to black but red (Sliding) is smoother, you might prefer Sliding for applications requiring smoothness over exact matching.

---

## 2. Smoothness Metrics Comparison

**File:** `smoothness_metrics_comparison.png`

### What It Shows

Bar charts comparing smoothness-related metrics across all methods. **Lower values are better** for all metrics.

### Metrics Displayed

1. **Mean Curvature**: Average amount of trajectory curvature
   - Lower = straighter, smoother paths
   - Higher = more curved, potentially jerky paths

2. **Max Curvature**: Maximum curvature value
   - Lower = no sharp turns
   - Higher = indicates sharp turns or sudden direction changes

3. **Mean Speed Change**: Average variation in speed
   - Lower = more consistent speed
   - Higher = more speed variations (acceleration/deceleration)

4. **Mean Yaw Rate**: Average rate of heading change
   - Lower = smoother heading transitions
   - Higher = more frequent or abrupt heading changes

### How to Interpret

- Compare bar heights: **Shorter bars = better smoothness**
- Look for consistency: Methods with similar bar heights across metrics are more balanced
- Check for outliers: One very high bar indicates a specific smoothness issue

### What to Look For

✅ **Good signs:**
- All bars are relatively low
- Consistent performance across all smoothness metrics
- No single metric with extremely high value

❌ **Warning signs:**
- Very tall bars (especially max curvature)
- Large differences between methods
- Inconsistent performance (some metrics good, others bad)

### Use Case

Choose the method with the lowest smoothness metrics if you need:
- Comfortable passenger experience
- Energy-efficient driving
- Natural-looking trajectories

---

## 3. Matching Metrics Comparison

**File:** `matching_metrics_comparison.png`

### What It Shows

Bar charts comparing how well recovered trajectories match the original reference trajectories. **Lower values are better** for all metrics.

### Metrics Displayed

1. **Endpoint Error**: Distance between trajectory endpoints
   - Lower = better endpoint accuracy
   - Higher = methods end at different locations

2. **Avg Displacement**: Average point-wise distance between trajectories
   - Lower = trajectories are closer overall
   - Higher = trajectories diverge more

3. **Max Displacement**: Maximum point-wise distance
   - Lower = no large deviations
   - Higher = indicates significant trajectory deviations

4. **Speed RMSE**: Root mean square error for speed profiles
   - Lower = speed profiles match better
   - Higher = speed differences between methods

5. **Yaw RMSE**: Root mean square error for heading/yaw profiles
   - Lower = heading profiles match better
   - Higher = heading differences between methods

### How to Interpret

- **Shorter bars = better matching** to the original trajectory
- Compare across metrics to see which method is most accurate overall
- Endpoint error is critical - even if other metrics are good, high endpoint error means the method doesn't reach the target

### What to Look For

✅ **Good signs:**
- Low endpoint error (most important)
- Low average displacement
- Consistent low values across all matching metrics

❌ **Warning signs:**
- High endpoint error (method doesn't reach target)
- Very high max displacement (large deviations)
- Inconsistent matching (some metrics good, others bad)

### Use Case

Choose the method with the lowest matching metrics if you need:
- Precise trajectory following
- Accurate goal reaching
- Faithful reproduction of expert demonstrations

---

## 4. Continuity Metrics Comparison

**File:** `continuity_metrics_comparison.png`

### What It Shows

Bar charts comparing how smoothly consecutive trajectory segments connect. **Lower values are better** for all metrics.

### Metrics Displayed

1. **Position Discontinuity**: Gap between consecutive segments
   - Lower = smooth position transitions
   - Higher = gaps or jumps between segments

2. **Velocity Discontinuity**: Speed change between segments
   - Lower = smooth speed transitions
   - Higher = sudden speed changes (jerky)

3. **Yaw Discontinuity**: Heading change between segments
   - Lower = smooth heading transitions
   - Higher = sudden direction changes

### How to Interpret

These metrics measure **segment-to-segment transitions**, not individual trajectory quality. Low values mean the recovered parameters create smooth, continuous paths when chained together.

### What to Look For

✅ **Good signs:**
- All bars are low (near zero)
- Smooth transitions between segments
- No sudden jumps or discontinuities

❌ **Warning signs:**
- High position discontinuity (visible gaps)
- High velocity discontinuity (jerky speed changes)
- High yaw discontinuity (sudden turns)

### Use Case

Choose the method with the lowest continuity metrics if you need:
- Long-term trajectory planning
- Smooth multi-segment paths
- Continuous driving without stops or jumps

---

## 5. Radar Chart Comparison

**File:** `radar_chart_comparison.png`

### What It Shows

Three radar charts (one for each category: Smoothness, Matching, Continuity) showing normalized metric values. Values are normalized to 0-1 scale for visualization.

### Visual Elements

- **Blue area**: Global Optimization method
- **Red area**: Sliding Window method
- **Green area**: Fast Method
- **Axes**: Each axis represents one metric
- **Center (0)**: Best possible value (after normalization)
- **Outer edge (1)**: Worst value (after normalization)

### How to Interpret

- **Smaller area = better performance** (closer to center)
- Compare shapes: Methods with similar shapes have similar performance profiles
- Look for balanced vs unbalanced: A method with one very long axis has a specific weakness

### What to Look For

✅ **Good signs:**
- Small, compact shapes (close to center)
- Balanced shapes (no extremely long axes)
- Consistent performance across all metrics

❌ **Warning signs:**
- Large areas (far from center)
- Unbalanced shapes (one very long axis)
- Inconsistent performance patterns

### Use Case

Use radar charts for:
- Quick visual comparison of overall performance
- Identifying which method is most balanced
- Spotting specific weaknesses in each method

---

## 6. Metrics Over Trajectories

**File:** `metrics_over_trajectories.png`

### What It Shows

Line plots showing how key metrics vary across different trajectory segments. Shows 4 subplots for different metrics.

### Metrics Displayed

1. **Endpoint Error Over Trajectories**: How endpoint accuracy varies
2. **Avg Displacement Over Trajectories**: How overall matching varies
3. **Mean Curvature Over Trajectories**: How smoothness varies
4. **Mean Speed Change Over Trajectories**: How speed consistency varies

### Visual Elements

- **Blue line with circles**: Global Optimization
- **Red line with squares**: Sliding Window
- **Green line with triangles**: Fast Method
- **X-axis**: Trajectory index (which trajectory segment)
- **Y-axis**: Metric value

### How to Interpret

- **Consistent lines**: Method performs similarly across all trajectories
- **Trending lines**: Method improves or degrades over time
- **Spikes**: Specific trajectories where method struggles
- **Convergence**: Methods performing similarly (lines close together)

### What to Look For

✅ **Good signs:**
- Stable, consistent lines (low variance)
- Lines close together (methods perform similarly)
- No large spikes or sudden changes

❌ **Warning signs:**
- High variance (lines jump around)
- Large spikes (method fails on specific trajectories)
- Diverging trends (methods getting worse over time)

### Use Case

Use this graph to:
- Identify problematic trajectory types
- Check method consistency
- Find trajectories where methods struggle

---

## 7. Summary Comparison

**File:** `summary_comparison.png`

### What It Shows

A 2×2 grid with 4 subplots providing an overall performance summary.

### Subplot 1: Trajectory Matching Error Comparison

Grouped bar chart comparing matching errors:
- **Metrics**: Endpoint error, avg displacement, speed RMSE, yaw RMSE
- **Lower bars = better matching**

### Subplot 2: Trajectory Smoothness Comparison

Grouped bar chart comparing smoothness:
- **Metrics**: Mean curvature, mean speed change, mean yaw rate
- **Lower bars = smoother trajectories**

### Subplot 3: Segment Continuity Comparison

Grouped bar chart comparing continuity:
- **Metrics**: Position, velocity, yaw discontinuity
- **Lower bars = smoother transitions**

### Subplot 4: Overall Performance Score

Single bar chart with composite score:
- **Weighted combination** of key metrics:
  - Endpoint error: 30%
  - Avg displacement: 20%
  - Mean curvature: 20%
  - Mean speed change: 15%
  - Position discontinuity: 15%
- **Lower score = better overall performance**

### How to Interpret

1. **Top-left (Matching)**: Which method matches the original best?
2. **Top-right (Smoothness)**: Which method produces smoothest trajectories?
3. **Bottom-left (Continuity)**: Which method has smoothest transitions?
4. **Bottom-right (Overall)**: **This is the key plot** - shows overall winner

### What to Look For

✅ **Good signs:**
- Consistent low bars across all subplots
- One method clearly winning in overall score
- Balanced performance (not just good in one area)

❌ **Warning signs:**
- High bars in any subplot
- Different winners in different categories
- Unbalanced performance (excellent in one area, poor in others)

### Use Case

**Start here** for a quick overall assessment. The bottom-right subplot gives you the composite winner, then check other subplots to understand why.

---

## How to Use These Graphs

### Step-by-Step Analysis Process

1. **Start with Summary Comparison** (`summary_comparison.png`)
   - Check the overall performance score (bottom-right)
   - Identify the winner

2. **Check Spatial Comparison** (`trajectory_spatial_comparison.png`)
   - Verify the winner visually
   - Look for any obvious issues

3. **Deep Dive into Specific Metrics**
   - If smoothness is important → Check `smoothness_metrics_comparison.png`
   - If accuracy is important → Check `matching_metrics_comparison.png`
   - If continuity is important → Check `continuity_metrics_comparison.png`

4. **Check Consistency** (`metrics_over_trajectories.png`)
   - Verify the winner performs consistently
   - Look for problematic trajectory types

5. **Quick Visual Check** (`radar_chart_comparison.png`)
   - Get an overall visual sense of performance balance

### Decision Making

**Choose Global Optimization if:**
- You need best trajectory matching accuracy
- Endpoint precision is critical
- You can tolerate slightly less smoothness

**Choose Sliding Window if:**
- You need balanced performance
- Smoothness and continuity are important
- You want consistent performance across trajectories

**Choose Fast Method if:**
- You need fast computation
- Good enough performance is acceptable
- You prioritize speed over perfection

---

## Common Patterns

### Pattern 1: Clear Winner
- One method has consistently lower bars across all graphs
- **Action**: Use that method

### Pattern 2: Trade-offs
- Method A is best at matching, Method B is best at smoothness
- **Action**: Choose based on your priority (accuracy vs smoothness)

### Pattern 3: All Similar
- All methods perform similarly
- **Action**: Choose based on computational cost or other factors

### Pattern 4: Inconsistent Performance
- Method performs well on some trajectories, poorly on others
- **Action**: Investigate problematic trajectory types, consider hybrid approach

---

## Tips for Interpretation

1. **Focus on your priorities**: If smoothness matters most, prioritize smoothness metrics
2. **Check endpoint error first**: High endpoint error often indicates fundamental issues
3. **Look for consistency**: A method that's consistently "good enough" may be better than one that's excellent sometimes and terrible other times
4. **Consider computational cost**: If performance is similar, choose the faster method
5. **Visual inspection matters**: Sometimes the spatial comparison reveals issues not captured by metrics

---

## Troubleshooting Graph Issues

### No Graphs Generated
- Check that all three recovery methods completed successfully
- Verify output directory exists and is writable
- Check console output for error messages

### Missing Metrics
- Some metrics may not be available if ground truth data is missing
- Check that demonstration data was collected properly

### Empty or Incomplete Graphs
- Ensure sufficient data was processed (at least a few trajectory files)
- Check that recovery methods produced valid results

---

## Next Steps

After analyzing the graphs:

1. **Document your findings**: Note which method performs best for your use case
2. **Use the best method**: For downstream tasks like actor pretraining
3. **Tune if needed**: If no method is satisfactory, consider adjusting recovery parameters
4. **Re-run comparison**: After making changes, regenerate graphs to verify improvements

---

## Quick Reference: Graph Checklist

When reviewing results, check:

- [ ] **Summary Comparison**: Overall winner identified?
- [ ] **Spatial Comparison**: Visual verification looks good?
- [ ] **Matching Metrics**: Endpoint error acceptable?
- [ ] **Smoothness Metrics**: Trajectories smooth enough?
- [ ] **Continuity Metrics**: Segments connect smoothly?
- [ ] **Metrics Over Time**: Performance consistent?
- [ ] **Radar Charts**: Overall balance acceptable?

If all checked, you're ready to use the best method for your application!
